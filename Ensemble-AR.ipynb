{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning for activity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.layers import GRU, LSTM, Activation, Bidirectional\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten \n",
    "from keras.layers import Dense, concatenate\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from numpy import dstack\n",
    "from attention import Attention\n",
    "\n",
    "# univariate cnn lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X: (340, 724)\n",
      "The shape of Y: (340, 4)\n"
     ]
    }
   ],
   "source": [
    "# load dataset with pandas\n",
    "dataframe1 = pd.read_csv('data/input_v2/Presentation_activity.csv', sep=',', header=None)\n",
    "dataframe2 = pd.read_csv('data/input_v2/Chatting_activity.csv', sep=',', header=None)\n",
    "dataframe3 = pd.read_csv('data/input_v2/Discussion_activity.csv', sep=',', header=None)\n",
    "dataframe4 = pd.read_csv('data/input_v2/GroupStudy_activity.csv', sep=',', header=None)\n",
    "\n",
    "# Show how many episodes \n",
    "#print(len(dataframe1))\n",
    "#print(len(dataframe2))\n",
    "#print(len(dataframe3))\n",
    "#print(len(dataframe4))\n",
    "\n",
    "#Combine all dataframe\n",
    "dataframe = dataframe1.append(dataframe2,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe3,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe4,ignore_index=True)\n",
    "\n",
    "#Change NaN data to 0\n",
    "dataframe = dataframe.fillna(0)\n",
    "dataset = dataframe.values\n",
    "\n",
    "#288, 6\n",
    "#Seperate input and do one-hot encoding\n",
    "#X=to_categorical(dataset)\n",
    "X=dataset\n",
    "print(\"The shape of X:\", X.shape)\n",
    "\n",
    "# Construct Y label\n",
    "arr = [] #  empty regular list\n",
    "for i in range(len(dataframe1)):\n",
    "    arr.append(0*np.ones((1)))\n",
    "for i in range(len(dataframe2)):\n",
    "    arr.append(1*np.ones((1)))\n",
    "for i in range(len(dataframe3)):\n",
    "    arr.append(2*np.ones((1)))\n",
    "for i in range(len(dataframe4)):\n",
    "    arr.append(3*np.ones((1)))\n",
    "np_array = np.array(arr)  # transformed to a numpy array\n",
    "y_df = pd.DataFrame({'': np_array[:, 0]})\n",
    "\n",
    "Y=to_categorical(y_df)\n",
    "#Y=y_df\n",
    "print(\"The shape of Y:\", Y.shape)\n",
    "\n",
    "\n",
    "# Divide train and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train DeepLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "length=724\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(length, 100, input_length=X.shape[1]))\n",
    "model1.add(SpatialDropout1D(0.2))\n",
    "model1.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model1.add(Dense(4, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/20\n",
      "Executing op __inference_keras_scratch_graph_262206 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "history = model1.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "# evaluate LSTM\n",
    "# evaluate model\n",
    "accr = model1.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "print(accr)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "yhat1=model1.predict(X_test,verbose=0)\n",
    "argmax(yhat1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(length, 100, input_length=X.shape[1]))\n",
    "model2.add(SpatialDropout1D(0.2))  \n",
    "model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model2.add(Dense(4, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "batch_size = 64\n",
    "# Divide train and test data\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "history = model2.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "# evaluate LSTM\n",
    "# evaluate model\n",
    "accr = model2.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "print(accr)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "yhat2=model2.predict(X_test,verbose=0)\n",
    "argmax(yhat2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(length, 100, input_length=X.shape[1]))\n",
    "model3.add(SpatialDropout1D(0.2))  \n",
    "model3.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model3.add(Dense(4, activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "# Divide train and test data\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "history = model3.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "# evaluate LSTM\n",
    "# evaluate model\n",
    "accr = model3.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "print(accr)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "yhat3=model3.predict(X_test,verbose=0)\n",
    "argmax(yhat3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(length, 100, input_length=X.shape[1]))\n",
    "model4.add(SpatialDropout1D(0.2))  \n",
    "model4.add(Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model4.add(Dense(4, activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "# Divide train and test data\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "history = model4.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "# evaluate LSTM\n",
    "# evaluate model\n",
    "accr = model4.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "print(accr)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "yhat4=model4.predict(X_test,verbose=0)\n",
    "argmax(yhat4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "n_seq = 4\n",
    "n_steps = 181\n",
    "# define model\n",
    "model5 = Sequential()\n",
    "model5.add(TimeDistributed(Conv1D(filters=4, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "model5.add(TimeDistributed(MaxPooling1D(pool_size=4)))\n",
    "model5.add(TimeDistributed(Flatten()))\n",
    "model5.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model5.add(Dense(4, activation='softmax'))\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "# fit model\n",
    "X_train1 = X_train.reshape((X_train.shape[0], n_seq, n_steps, n_features))\n",
    "model5.fit(X_train1, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "# demonstrate prediction    \n",
    "X_test1 = X_test.reshape((X_test.shape[0], n_seq, n_steps, n_features))\n",
    "accr = model5.evaluate(X_test1, Y_test, batch_size=batch_size, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "yhat5 = model5.predict(X_test1, verbose=0)\n",
    "print(yhat5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import ConvLSTM2D\n",
    "n_features = 1\n",
    "n_seq = 4\n",
    "n_steps = 181\n",
    "# define model\n",
    "model6 = Sequential()\n",
    "model6.add(ConvLSTM2D(filters=4, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "model6.add(TimeDistributed(Flatten()))\n",
    "model6.add(Dense(4, activation='softmax'))\n",
    "model6.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "# fit model\n",
    "X_train2 = X_train.reshape((X_train.shape[0], n_seq, 1, n_steps, n_features))\n",
    "Y_train2 = Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))\n",
    "model6.fit(X_train2, Y_train2, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "# demonstrate prediction    \n",
    "X_test2 = X_test.reshape((X_test.shape[0], n_seq, 1, n_steps, n_features))\n",
    "Y_test2 = Y_test.reshape((Y_test.shape[0], 1, Y_test.shape[1]))\n",
    "accr = model6.evaluate(X_test2, Y_test2, batch_size=batch_size, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "yhat6 = model6.predict(X_test2, verbose=0)\n",
    "print(yhat6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    " \n",
    "# create directory for models\n",
    "#makedirs('models')\n",
    "\n",
    "# save model\n",
    "filename = 'models/model_1.h5'\n",
    "model1.save(filename)\n",
    "print('>Saved %s' % filename)\n",
    "\n",
    "filename = 'models/model_2.h5'\n",
    "model2.save(filename)\n",
    "print('>Saved %s' % filename)\n",
    "\n",
    "filename = 'models/model_3.h5'\n",
    "model3.save(filename)\n",
    "print('>Saved %s' % filename)\n",
    "\n",
    "filename = 'models/model_4.h5'\n",
    "model4.save(filename)\n",
    "print('>Saved %s' % filename)\n",
    "\n",
    "filename = 'models/model_5.h5'\n",
    "model5.save(filename)\n",
    "print('>Saved %s' % filename)\n",
    "\n",
    "filename = 'models/model_6.h5'\n",
    "model6.save(filename)\n",
    "print('>Saved %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = [(yhat1[i] + yhat2[i]) for i in range(len(yhat2))] \n",
    "res_list\n",
    "r1=argmax(res_list, axis=1)\n",
    "print(r1)\n",
    "r2=argmax(Y_test, axis=1)\n",
    "print(len(r2))\n",
    "count=0\n",
    "for j in range(len(r2)):\n",
    "   if r1[j]==r2[j] :\n",
    "        count+=1\n",
    "print(count/len(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat6 = np.reshape(yhat6,(len(yhat6),4))\n",
    "res_list = [(yhat1[i] + yhat2[i] + yhat3[i]+yhat5[i]+yhat6[i]) for i in range(len(yhat2))] \n",
    "res_list\n",
    "r1=argmax(res_list, axis=1)\n",
    "print(r1)\n",
    "r2=argmax(Y_test, axis=1)\n",
    "print(len(r2))\n",
    "count=0\n",
    "for j in range(len(r2)):\n",
    "   if r1[j]==r2[j] :\n",
    "    count=count+1\n",
    "print(count/len(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_rs = 0*len(Y_test)\n",
    "true_rs = argmax(Y_test, axis=1)\n",
    "print(true_rs)\n",
    "pre_rs = np.full(( len(Y_test),4),0)\n",
    "rs1 =  argmax(yhat1, axis=1)\n",
    "rs2 =  argmax(yhat2, axis=1)\n",
    "rs3 =  argmax(yhat3, axis=1)\n",
    "rs4 =  argmax(yhat4, axis=1)\n",
    "rs5 =  argmax(yhat5, axis=1)\n",
    "rs6 =  argmax(yhat6, axis=1)\n",
    "\n",
    "i=0\n",
    "for i in range(len(Y_test)):\n",
    "    pre_rs[i][rs1[i]]+= 1\n",
    "    #pre_rs[i][rs2[i]]+= 1\n",
    "    #pre_rs[i][rs3[i]]+= 1\n",
    "    pre_rs[i][rs4[i]]+= 1\n",
    "    pre_rs[i][rs5[i]]+= 1\n",
    "    pre_rs[i][rs6[i]]+= 1 \n",
    "    \n",
    "count=0\n",
    "rs_final = argmax(pre_rs, axis=1)\n",
    "print(rs_final)\n",
    "for j in range(len(true_rs)):\n",
    "   if true_rs[j]== rs_final[j] :\n",
    "        count+=1\n",
    "print(count/len(true_rs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from itertools import product\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "\t# calculate l1 vector norm\n",
    "\tresult = norm(weights, 1)\n",
    "\t# check for a vector of all zeros\n",
    "\tif result == 0.0:\n",
    "\t\treturn weights\n",
    "\t# return normalized vector (unit norm)\n",
    "\treturn weights / result\n",
    "r2=argmax(Y_test, axis=1)\n",
    "members=[model1,model2,model3,model4,model5,model6]\n",
    "yhats=[yhat1, yhat2, yhat3, yhat4, yhat5, yhat6]\n",
    "# define weights to consider\n",
    "w = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "best_score, best_weights = 0.0, None\n",
    "for weights in product(w, repeat=len(members)):\n",
    "\t# skip if all weights are equal\n",
    "\tif len(set(weights)) == 1:\n",
    "\t\tcontinue\n",
    "# hack, normalize weight vector\n",
    "\tweights = normalize(weights)\n",
    "# evaluate weights\n",
    "\tyhats =  array(yhats)\n",
    "\tsummed =  tensordot(yhats, weights, axes=((0),(0)))\n",
    "\tresult = argmax(summed, axis=1)\n",
    "\tscore =  accuracy_score(r2, result)\n",
    "\tif score > best_score:\n",
    "\t\tbest_score, best_weights = score, weights\n",
    "\t\tprint('>%s %.3f' % (best_weights, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0-gpu",
   "language": "python",
   "name": "tf2.0-gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

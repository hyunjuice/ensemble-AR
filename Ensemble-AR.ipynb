{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning for activity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.layers import GRU, LSTM, Activation, Bidirectional\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten \n",
    "from keras.layers import Dense, concatenate\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from numpy import dstack\n",
    "from attention import Attention\n",
    "\n",
    "# univariate cnn lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import ConvLSTM2D\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X: (340, 724)\n",
      "The shape of Y: (340, 4)\n"
     ]
    }
   ],
   "source": [
    "# load dataset with pandas\n",
    "dataframe1 = pd.read_csv('data/input_v2/Presentation_activity.csv', sep=',', header=None)\n",
    "dataframe2 = pd.read_csv('data/input_v2/Chatting_activity.csv', sep=',', header=None)\n",
    "dataframe3 = pd.read_csv('data/input_v2/Discussion_activity.csv', sep=',', header=None)\n",
    "dataframe4 = pd.read_csv('data/input_v2/GroupStudy_activity.csv', sep=',', header=None)\n",
    "\n",
    "# Show how many episodes \n",
    "#print(len(dataframe1))\n",
    "#print(len(dataframe2))\n",
    "#print(len(dataframe3))\n",
    "#print(len(dataframe4))\n",
    "\n",
    "#Combine all dataframe\n",
    "dataframe = dataframe1.append(dataframe2,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe3,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe4,ignore_index=True)\n",
    "\n",
    "#Change NaN data to 0\n",
    "dataframe = dataframe.fillna(0)\n",
    "dataset = dataframe.values\n",
    "\n",
    "#288, 6\n",
    "#Seperate input and do one-hot encoding\n",
    "#X=to_categorical(dataset)\n",
    "X=dataset\n",
    "print(\"The shape of X:\", X.shape)\n",
    "\n",
    "# Construct Y label\n",
    "arr = [] #  empty regular list\n",
    "for i in range(len(dataframe1)):\n",
    "    arr.append(0*np.ones((1)))\n",
    "for i in range(len(dataframe2)):\n",
    "    arr.append(1*np.ones((1)))\n",
    "for i in range(len(dataframe3)):\n",
    "    arr.append(2*np.ones((1)))\n",
    "for i in range(len(dataframe4)):\n",
    "    arr.append(3*np.ones((1)))\n",
    "np_array = np.array(arr)  # transformed to a numpy array\n",
    "y_df = pd.DataFrame({'': np_array[:, 0]})\n",
    "\n",
    "Y=to_categorical(y_df)\n",
    "#Y=y_df\n",
    "print(\"The shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train DeepLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_408665 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 29s - loss: 1.3735 - accuracy: 0.3698 Executing op __inference_keras_scratch_graph_408999 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 133s 547ms/step - loss: 1.3657 - accuracy: 0.3770 - val_loss: 1.3305 - val_accuracy: 0.2143\n",
      "Test set\n",
      "  Loss: 1.341\n",
      "  Accuracy: 0.368\n",
      "Executing op __inference_keras_scratch_graph_409321 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_410847 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 8s - loss: 1.3726 - accuracy: 0.3438 Executing op __inference_keras_scratch_graph_411059 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 69s 282ms/step - loss: 1.3682 - accuracy: 0.3566 - val_loss: 1.3481 - val_accuracy: 0.2143\n",
      "Test set\n",
      "  Loss: 1.342\n",
      "  Accuracy: 0.368\n",
      "Executing op __inference_keras_scratch_graph_411259 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_412888 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 9s - loss: 1.3619 - accuracy: 0.3698 Executing op __inference_keras_scratch_graph_413112 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 48s 198ms/step - loss: 1.3593 - accuracy: 0.3811 - val_loss: 1.3334 - val_accuracy: 0.2143\n",
      "Test set\n",
      "  Loss: 1.345\n",
      "  Accuracy: 0.368\n",
      "Executing op __inference_keras_scratch_graph_413324 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_416021 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 21s - loss: 1.3613 - accuracy: 0.3750 Executing op __inference_keras_scratch_graph_416379 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 104s 428ms/step - loss: 1.3530 - accuracy: 0.3730 - val_loss: 1.3097 - val_accuracy: 0.2143\n",
      "Test set\n",
      "  Loss: 1.336\n",
      "  Accuracy: 0.368\n",
      "Executing op __inference_keras_scratch_graph_416725 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_419313 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 13s - loss: 1.3632 - accuracy: 0.2708 Executing op __inference_keras_scratch_graph_419690 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 68s 277ms/step - loss: 1.3378 - accuracy: 0.3074 - val_loss: 1.3432 - val_accuracy: 0.2143\n",
      "Test set\n",
      "  Loss: 1.271\n",
      "  Accuracy: 0.368\n",
      "Executing op __inference_keras_scratch_graph_420055 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_421678 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      " 64/244 [======>.......................] - ETA: 1:13 - loss: 1.3906 - accuracy: 0.1875Executing op __inference_keras_scratch_graph_421933 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 36s 149ms/step - loss: 1.3719 - accuracy: 0.4139 - val_loss: 1.3646 - val_accuracy: 0.3571\n",
      "Test set\n",
      "  Loss: 1.357\n",
      "  Accuracy: 0.397\n",
      "Executing op __inference_keras_scratch_graph_422176 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_424272 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "length=724\n",
    "n_members=7\n",
    "\n",
    "def model1_made():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Embedding(length, 100, input_length=X.shape[1]))\n",
    "    model1.add(SpatialDropout1D(0.2))\n",
    "    model1.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model1.add(Dense(4, activation='softmax'))\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model1\n",
    "def model2_made():\n",
    "    model2 = Sequential()\n",
    "    model2.add(Embedding(length, 100, input_length=X.shape[1]))\n",
    "    model2.add(SpatialDropout1D(0.2))  \n",
    "    model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model2.add(Dense(4, activation='softmax'))\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model2\n",
    "def model3_made():\n",
    "    model3 = Sequential()\n",
    "    model3.add(Embedding(length, 100, input_length=X.shape[1]))\n",
    "    model3.add(SpatialDropout1D(0.2))  \n",
    "    model3.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model3.add(Dense(4, activation='softmax'))\n",
    "    model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model3\n",
    "def model4_made():\n",
    "    model4 = Sequential()\n",
    "    model4.add(Embedding(length, 100, input_length=X.shape[1]))\n",
    "    model4.add(SpatialDropout1D(0.2))  \n",
    "    model4.add(Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model4.add(Dense(4, activation='softmax'))\n",
    "    model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model4\n",
    "def model5_made():\n",
    "    n_features = 1\n",
    "    n_seq = 4\n",
    "    n_steps = 181\n",
    "    # define model\n",
    "    model5 = Sequential()\n",
    "    model5.add(TimeDistributed(Conv1D(filters=4, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "    model5.add(TimeDistributed(MaxPooling1D(pool_size=4)))\n",
    "    model5.add(TimeDistributed(Flatten()))\n",
    "    model5.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model5.add(Dense(4, activation='softmax'))\n",
    "    model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model5\n",
    "def model6_made():\n",
    "    n_features = 1\n",
    "    n_seq = 4\n",
    "    n_steps = 181\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=4, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=4)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model7_made():\n",
    "    n_features = 1\n",
    "    n_seq = 4\n",
    "    n_steps = 181\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=4, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def model_evaluate1(X_train,Y_train,model,X_test, Y_test):\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # evaluate model\n",
    "    accr = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    #plt.title('Loss')\n",
    "    #plt.plot(history.history['loss'], label='train')\n",
    "    #plt.plot(history.history['val_loss'], label='test')\n",
    "    #plt.legend()\n",
    "    #plt.show();\n",
    "\n",
    "    #plt.title('Accuracy')\n",
    "    #plt.plot(history.history['accuracy'], label='train')\n",
    "    #plt.plot(history.history['val_accuracy'], label='test')\n",
    "    #plt.legend()\n",
    "    #plt.show();\n",
    "\n",
    "    yhat=model.predict(X_test,verbose=0)\n",
    "    #argmax(yhat1, axis=1)\n",
    "    return [accr, yhat]\n",
    "def model_evaluate2(X_train,Y_train,model,X_test, Y_test):\n",
    "    n_features = 1\n",
    "    n_seq = 4\n",
    "    n_steps = 181\n",
    "    # fit model\n",
    "    X_train1 = X_train.reshape((X_train.shape[0], n_seq, n_steps, n_features))\n",
    "    model.fit(X_train1, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # demonstrate prediction    \n",
    "    X_test1 = X_test.reshape((X_test.shape[0], n_seq, n_steps, n_features))\n",
    "    accr = model.evaluate(X_test1, Y_test, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    yhat = model.predict(X_test1, verbose=0)\n",
    "    return [accr, yhat]\n",
    "def model_evaluate3(X_train,Y_train,model,X_test, Y_test):    \n",
    "    n_features = 1\n",
    "    n_seq = 4\n",
    "    n_steps = 181\n",
    "    # fit model\n",
    "    X_train2 = X_train.reshape((X_train.shape[0], n_seq, 1, n_steps, n_features))\n",
    "    Y_train2 = Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))\n",
    "    model.fit(X_train2, Y_train2, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # demonstrate prediction    \n",
    "    X_test2 = X_test.reshape((X_test.shape[0], n_seq, 1, n_steps, n_features))\n",
    "    Y_test2 = Y_test.reshape((Y_test.shape[0], 1, Y_test.shape[1]))\n",
    "    accr = model.evaluate(X_test2, Y_test2, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "        \n",
    "    yhat = model.predict(X_test2, verbose=0)\n",
    "    yhat = np.reshape(yhat,(len(yhat),4))\n",
    "    return [accr, yhat]\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "\t# calculate l1 vector norm\n",
    "\tresult = norm(weights, 1)\n",
    "\t# check for a vector of all zeros\n",
    "\tif result == 0.0:\n",
    "\t\treturn weights\n",
    "\t# return normalized vector (unit norm)\n",
    "\treturn weights / result\n",
    "\n",
    "def top2ensemble(accr,yhats,Y_test):\n",
    "    largest,second_largest=0,0\n",
    "    for i in range(len(accr)):\n",
    "        if accr[i] > accr[largest]:\n",
    "            largest = i\n",
    "        elif accr[largest] > accr[i] > accr[second_largest]:\n",
    "                second_largest = i\n",
    "\n",
    "    pre_rs = [(yhats[largest][i] + yhats[second_largest][i]) for i in range(len(yhats[0]))]     \n",
    "    pre_rs = argmax(pre_rs, axis=1)    \n",
    "    true_rs=argmax(Y_test, axis=1)\n",
    "    count=0\n",
    "    for j in range(len(true_rs)):\n",
    "        if pre_rs[j]==true_rs[j] :\n",
    "            count+=1       \n",
    "    print('Top2Ensemble accuracy : ', count/len(true_rs))    \n",
    "\n",
    "def ensempb(accr,yhats,Y_test):\n",
    "    pre_rs = [(yhats[0][i] + yhats[1][i] +yhats[2][i] +yhats[3][i]+yhats[4][i]+yhats[5][i]+yhats[6][i]) for i in range(len(yhats[0]))] \n",
    "    pre_rs=argmax(pre_rs, axis=1)\n",
    "    true_rs=argmax(Y_test, axis=1)    \n",
    "    count=0\n",
    "    for j in range(len(true_rs)):\n",
    "        if pre_rs[j]==true_rs[j] :\n",
    "            count+=1\n",
    "    print('Probability ensemble accuracy : ', count/len(true_rs))\n",
    "def ensemvote(yhats,Y_test):\n",
    "    pre_rs = np.full((len(Y_test),4),0)\n",
    "    rs = np.zeros((n_members, len(Y_test)))\n",
    "    for i in range(n_members):\n",
    "        rs[i]= argmax(yhats[i], axis=1)\n",
    "\n",
    "    for i in range(n_members):\n",
    "        for j in range(len(Y_test)):\n",
    "            pre_rs[j][int(rs[i][j])]+=1\n",
    "    pre_rs = argmax(pre_rs, axis=1)\n",
    "    true_rs = argmax(Y_test, axis=1)\n",
    "\n",
    "    count=0\n",
    "    for j in range(len(true_rs)):\n",
    "        if pre_rs[j]==true_rs[j] :\n",
    "            count+=1\n",
    "    print('Voting ensemble accuracy : ', count/len(true_rs))   \n",
    "    \n",
    "def ensemweighted(accr,yhats,Y_test):\n",
    "    r2=argmax(Y_test, axis=1)\n",
    "    # define weights to consider\n",
    "    w = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    best_score, best_weights = 0.0, None\n",
    "    for weights in product(w, repeat=len(members)):\n",
    "        # skip if all weights are equal\n",
    "        if len(set(weights)) == 1:\n",
    "            continue\n",
    "    # hack, normalize weight vector\n",
    "        weights = normalize(weights)\n",
    "    # evaluate weights\n",
    "        yhats =  array(yhats)\n",
    "        summed =  tensordot(yhats, weights, axes=((0),(0)))\n",
    "        result = argmax(summed, axis=1)\n",
    "        score =  accuracy_score(r2, result)\n",
    "        if score > best_score:\n",
    "            best_score, best_weights = score, weights\n",
    "    \n",
    "    print('>%s %.3f' % (best_weights, best_score))\n",
    "for k in range(10):\n",
    "    accr=[]    \n",
    "    # Divide train and test data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    members=[model1_made(),model2_made(),model3_made(),model4_made(),model5_made(),model6_made(),model7_made()]\n",
    "    result =[model_evaluate1(X_train,Y_train,members[0],X_test, Y_test),\n",
    "           model_evaluate1(X_train,Y_train,members[1],X_test, Y_test),\n",
    "           model_evaluate1(X_train,Y_train,members[2],X_test, Y_test),\n",
    "           model_evaluate1(X_train,Y_train,members[3],X_test, Y_test),\n",
    "           model_evaluate2(X_train,Y_train,members[4],X_test, Y_test),\n",
    "           model_evaluate2(X_train,Y_train,members[5],X_test, Y_test),\n",
    "           model_evaluate3(X_train,Y_train,members[6],X_test, Y_test)]\n",
    "    \n",
    "    yhats= np.zeros((len(result), len(Y_test), 4))    \n",
    "    for i in range(len(result)):\n",
    "        yhats[i]=result[i][1]\n",
    "        accr.append(result[i][0][1])\n",
    "    print('Base Model Accuracy : ',accr)\n",
    "\n",
    "    top2ensemble(accr,yhats,Y_test)\n",
    "    ensempb(accr,yhats,Y_test)\n",
    "    ensemvote(yhats,Y_test)\n",
    "    ensemweighted(accr,yhats,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.3042122  0.2803825  0.21704166 0.19836357]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704169 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.3042122  0.2803825  0.21704167 0.19836357]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704169 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704167 0.1983636 ]\n",
      " [0.30421224 0.2803825  0.21704167 0.19836359]\n",
      " [0.30421224 0.2803825  0.21704169 0.1983636 ]]\n",
      "Y [[0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421221 0.28038251 0.21704166 0.19836357]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704169 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421221 0.28038251 0.21704167 0.19836357]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704169 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704167 0.1983636 ]\n",
      " [0.30421224 0.28038251 0.21704167 0.19836359]\n",
      " [0.30421224 0.28038251 0.21704169 0.1983636 ]]\n"
     ]
    }
   ],
   "source": [
    "print(result[1][1])\n",
    "yhats= np.zeros((len(result), len(Y_test), 4))\n",
    "for i in range(len(result)):\n",
    "    yhats[i]=result[i][1]\n",
    "    accr.append(result[i][0][1])\n",
    "print(\"Y\", yhats[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 2 0 0 2 0 0 2 0 0 1 0 1 0 0 1 0 0 1 0 2 2 0 0 3 1 0 1 1 1 1 0 3 1 0 1 3\n",
      " 3 2 0 0 1 2 2 2 0 1 0 2 1 1 1 2 1 2 1 0 3 3 1 0 0 0 1 1 1 0 2]\n",
      "0.39705882352941174\n"
     ]
    }
   ],
   "source": [
    "largest,second_largest=0,0\n",
    "for i in range(len(accr)):\n",
    "    if accr[i] > accr[largest]:\n",
    "        largest = i\n",
    "    elif accr[largest] > accr[i] > accr[second_largest]:\n",
    "            second_largest = i\n",
    "\n",
    "pre_rs = [(yhats[largest][i] + yhats[second_largest][i]) for i in range(len(yhats[0]))]     \n",
    "pre_rs = argmax(pre_rs, axis=1)    \n",
    "print(pre_rs)\n",
    "    \n",
    "true_rs=argmax(Y_test, axis=1)\n",
    "print(true_rs)\n",
    "    \n",
    "count=0\n",
    "for j in range(len(true_rs)):\n",
    "    if pre_rs[j]==true_rs[j] :\n",
    "        count+=1       \n",
    "\n",
    "print(count/len(true_rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0]\n",
      "[2 0 1 0 2 1 0 3 3 2 3 0 2 1 2 1 3 0 1 1 0 3 2 0 1 0 3 0 0 0 0 1 0 1 1 0 2\n",
      " 1 1 1 0 0 1 0 3 3 0 1 1 0 1 0 0 0 2 0 2 0 0 1 1 1 0 2 1 0 1 1]\n",
      "0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "pre_rs = [(yhats[0][i] + yhats[1][i] +yhats[2][i] +yhats[3][i]+yhats[4][i]+yhats[5][i]) for i in range(len(yhats[0]))] \n",
    "\n",
    "pre_rs=argmax(pre_rs, axis=1)\n",
    "print(pre_rs)\n",
    "\n",
    "true_rs=argmax(Y_test, axis=1)\n",
    "print(true_rs)\n",
    "\n",
    "count=0\n",
    "for j in range(len(true_rs)):\n",
    "    if pre_rs[j]==true_rs[j] :\n",
    "        count+=1\n",
    "print(count/len(true_rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting ensemble accuracy :  0.5147058823529411\n"
     ]
    }
   ],
   "source": [
    "pre_rs = np.full((len(Y_test),4),0)\n",
    "rs = np.zeros((n_members, len(Y_test)))\n",
    "for i in range(n_members):\n",
    "    rs[i]= argmax(yhats[i], axis=1)\n",
    "        \n",
    "for i in range(n_members):\n",
    "    for j in range(len(Y_test)):\n",
    "        pre_rs[j][int(rs[i][j])]+=1\n",
    "pre_rs = argmax(pre_rs, axis=1)\n",
    "true_rs = argmax(Y_test, axis=1)\n",
    "\n",
    "count=0\n",
    "for j in range(len(true_rs)):\n",
    "    if pre_rs[j]==true_rs[j] :\n",
    "        count+=1\n",
    "print('Voting ensemble accuracy : ', count/len(true_rs))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">[0. 0. 0. 0. 0. 1.] 0.397\n",
      ">[0. 0. 0. 0. 1. 0.] 0.500\n",
      ">[0.         0.         0.         0.         0.14285714 0.85714286] 0.515\n",
      ">[0.         0.         0.         0.14285714 0.14285714 0.71428571] 0.529\n",
      ">[0.         0.         0.         0.28571429 0.07142857 0.64285714] 0.544\n",
      ">[0.     0.     0.     0.3125 0.0625 0.625 ] 0.559\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "\t# calculate l1 vector norm\n",
    "\tresult = norm(weights, 1)\n",
    "\t# check for a vector of all zeros\n",
    "\tif result == 0.0:\n",
    "\t\treturn weights\n",
    "\t# return normalized vector (unit norm)\n",
    "\treturn weights / result\n",
    "r2=argmax(Y_test, axis=1)\n",
    "\n",
    "# define weights to consider\n",
    "w = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "best_score, best_weights = 0.0, None\n",
    "for weights in product(w, repeat=len(members)):\n",
    "\t# skip if all weights are equal\n",
    "\tif len(set(weights)) == 1:\n",
    "\t\tcontinue\n",
    "# hack, normalize weight vector\n",
    "\tweights = normalize(weights)\n",
    "# evaluate weights\n",
    "\tyhats =  array(yhats)\n",
    "\tsummed =  tensordot(yhats, weights, axes=((0),(0)))\n",
    "\tresult = argmax(summed, axis=1)\n",
    "\tscore =  accuracy_score(r2, result)\n",
    "\tif score > best_score:\n",
    "\t\tbest_score, best_weights = score, weights\n",
    "\t\tprint('>%s %.3f' % (best_weights, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9703461  0.00156423 0.02508036 0.0030093 ]\n",
      " [0.42118007 0.15806383 0.2412047  0.17955144]\n",
      " [0.97902006 0.00119417 0.01827183 0.00151398]\n",
      " [0.1516933  0.1005991  0.15920472 0.5885029 ]\n",
      " [0.1796446  0.45726845 0.18102014 0.18206684]\n",
      " [0.7491783  0.02688581 0.1503759  0.07355996]\n",
      " [0.9671414  0.00214818 0.02750238 0.00320814]\n",
      " [0.03979389 0.85280555 0.06361634 0.04378427]\n",
      " [0.13650976 0.13733433 0.1701387  0.5560172 ]\n",
      " [0.38114837 0.37429872 0.18178375 0.06276917]\n",
      " [0.04571417 0.8450254  0.06493264 0.04432774]\n",
      " [0.04012959 0.8575689  0.06155693 0.0407446 ]\n",
      " [0.96610236 0.00188214 0.0282616  0.00375381]\n",
      " [0.36849678 0.1700907  0.23345894 0.22795364]\n",
      " [0.08995757 0.7320923  0.10178467 0.07616542]\n",
      " [0.9712006  0.001466   0.02463255 0.00270091]\n",
      " [0.53981906 0.19393656 0.20256698 0.06367737]\n",
      " [0.14089054 0.585458   0.13953677 0.1341147 ]\n",
      " [0.05769477 0.81417954 0.07395729 0.05416843]\n",
      " [0.1236949  0.07716835 0.15380315 0.6453335 ]\n",
      " [0.06335168 0.8082822  0.07982234 0.04854372]\n",
      " [0.7070069  0.02754386 0.15370363 0.11174567]\n",
      " [0.9673981  0.00179106 0.02759837 0.00321241]\n",
      " [0.13097717 0.5482799  0.16797437 0.15276857]\n",
      " [0.6175176  0.12457525 0.185125   0.0727822 ]\n",
      " [0.97043085 0.00166571 0.02457877 0.00332463]\n",
      " [0.9715587  0.00140258 0.0238844  0.00315432]\n",
      " [0.75599265 0.05841357 0.14639945 0.03919425]\n",
      " [0.14145288 0.5615187  0.14919002 0.14783838]\n",
      " [0.956979   0.00237032 0.03468528 0.00596537]\n",
      " [0.06457947 0.80286604 0.08031181 0.05224257]\n",
      " [0.08483058 0.75737476 0.09778001 0.06001461]\n",
      " [0.1389336  0.62168497 0.13806632 0.1013151 ]\n",
      " [0.3915946  0.17129272 0.25041473 0.18669796]\n",
      " [0.51574    0.2708395  0.16925064 0.04416981]\n",
      " [0.61261564 0.09901236 0.1840867  0.10428531]\n",
      " [0.13059686 0.6089793  0.13611124 0.12431259]\n",
      " [0.53673077 0.24812801 0.17293927 0.04220195]\n",
      " [0.5569178  0.05310545 0.20449528 0.18548147]\n",
      " [0.1204346  0.6492622  0.12977974 0.10052347]\n",
      " [0.05847711 0.8150336  0.07594695 0.05054227]\n",
      " [0.05918564 0.8130368  0.07486349 0.05291405]\n",
      " [0.16464664 0.56539327 0.1576288  0.11233123]\n",
      " [0.9715921  0.00145732 0.02438533 0.00256522]\n",
      " [0.1606377  0.19089285 0.22262761 0.42584187]\n",
      " [0.47228196 0.20147027 0.22501904 0.10122871]\n",
      " [0.51208806 0.2171649  0.19790974 0.07283729]\n",
      " [0.49417484 0.0530624  0.17600818 0.2767546 ]\n",
      " [0.25975078 0.11972722 0.22209626 0.3984258 ]\n",
      " [0.05215175 0.8222115  0.07323202 0.05240474]\n",
      " [0.91224736 0.00974044 0.06678426 0.01122794]\n",
      " [0.56449217 0.12772839 0.21423455 0.09354493]\n",
      " [0.15771988 0.16402103 0.19932565 0.47893348]\n",
      " [0.07457786 0.7540506  0.09870991 0.07266164]\n",
      " [0.4024237  0.40386826 0.15433417 0.03937383]\n",
      " [0.25728902 0.23898083 0.2072349  0.2964952 ]\n",
      " [0.93807375 0.00465487 0.04918053 0.00809077]\n",
      " [0.04183402 0.8432126  0.06687161 0.04808175]\n",
      " [0.17549194 0.20994031 0.21033329 0.4042345 ]\n",
      " [0.1965938  0.28494465 0.22086123 0.29760033]\n",
      " [0.04252256 0.84881586 0.06601896 0.0426426 ]\n",
      " [0.19249792 0.42889595 0.19648264 0.18212341]\n",
      " [0.94688976 0.00335273 0.04188817 0.00786935]\n",
      " [0.977047   0.00121129 0.01970157 0.00204011]\n",
      " [0.9676783  0.00185347 0.02750921 0.00295907]\n",
      " [0.27091506 0.46448445 0.18088451 0.08371601]\n",
      " [0.33639503 0.14260666 0.25988573 0.26111263]\n",
      " [0.88110745 0.01994448 0.08073334 0.01821473]]\n",
      "[array([[0.9703461 , 0.00156423, 0.02508036, 0.0030093 ],\n",
      "       [0.42118007, 0.15806383, 0.2412047 , 0.17955144],\n",
      "       [0.97902006, 0.00119417, 0.01827183, 0.00151398],\n",
      "       [0.1516933 , 0.1005991 , 0.15920472, 0.5885029 ],\n",
      "       [0.1796446 , 0.45726845, 0.18102014, 0.18206684],\n",
      "       [0.7491783 , 0.02688581, 0.1503759 , 0.07355996],\n",
      "       [0.9671414 , 0.00214818, 0.02750238, 0.00320814],\n",
      "       [0.03979389, 0.85280555, 0.06361634, 0.04378427],\n",
      "       [0.13650976, 0.13733433, 0.1701387 , 0.5560172 ],\n",
      "       [0.38114837, 0.37429872, 0.18178375, 0.06276917],\n",
      "       [0.04571417, 0.8450254 , 0.06493264, 0.04432774],\n",
      "       [0.04012959, 0.8575689 , 0.06155693, 0.0407446 ],\n",
      "       [0.96610236, 0.00188214, 0.0282616 , 0.00375381],\n",
      "       [0.36849678, 0.1700907 , 0.23345894, 0.22795364],\n",
      "       [0.08995757, 0.7320923 , 0.10178467, 0.07616542],\n",
      "       [0.9712006 , 0.001466  , 0.02463255, 0.00270091],\n",
      "       [0.53981906, 0.19393656, 0.20256698, 0.06367737],\n",
      "       [0.14089054, 0.585458  , 0.13953677, 0.1341147 ],\n",
      "       [0.05769477, 0.81417954, 0.07395729, 0.05416843],\n",
      "       [0.1236949 , 0.07716835, 0.15380315, 0.6453335 ],\n",
      "       [0.06335168, 0.8082822 , 0.07982234, 0.04854372],\n",
      "       [0.7070069 , 0.02754386, 0.15370363, 0.11174567],\n",
      "       [0.9673981 , 0.00179106, 0.02759837, 0.00321241],\n",
      "       [0.13097717, 0.5482799 , 0.16797437, 0.15276857],\n",
      "       [0.6175176 , 0.12457525, 0.185125  , 0.0727822 ],\n",
      "       [0.97043085, 0.00166571, 0.02457877, 0.00332463],\n",
      "       [0.9715587 , 0.00140258, 0.0238844 , 0.00315432],\n",
      "       [0.75599265, 0.05841357, 0.14639945, 0.03919425],\n",
      "       [0.14145288, 0.5615187 , 0.14919002, 0.14783838],\n",
      "       [0.956979  , 0.00237032, 0.03468528, 0.00596537],\n",
      "       [0.06457947, 0.80286604, 0.08031181, 0.05224257],\n",
      "       [0.08483058, 0.75737476, 0.09778001, 0.06001461],\n",
      "       [0.1389336 , 0.62168497, 0.13806632, 0.1013151 ],\n",
      "       [0.3915946 , 0.17129272, 0.25041473, 0.18669796],\n",
      "       [0.51574   , 0.2708395 , 0.16925064, 0.04416981],\n",
      "       [0.61261564, 0.09901236, 0.1840867 , 0.10428531],\n",
      "       [0.13059686, 0.6089793 , 0.13611124, 0.12431259],\n",
      "       [0.53673077, 0.24812801, 0.17293927, 0.04220195],\n",
      "       [0.5569178 , 0.05310545, 0.20449528, 0.18548147],\n",
      "       [0.1204346 , 0.6492622 , 0.12977974, 0.10052347],\n",
      "       [0.05847711, 0.8150336 , 0.07594695, 0.05054227],\n",
      "       [0.05918564, 0.8130368 , 0.07486349, 0.05291405],\n",
      "       [0.16464664, 0.56539327, 0.1576288 , 0.11233123],\n",
      "       [0.9715921 , 0.00145732, 0.02438533, 0.00256522],\n",
      "       [0.1606377 , 0.19089285, 0.22262761, 0.42584187],\n",
      "       [0.47228196, 0.20147027, 0.22501904, 0.10122871],\n",
      "       [0.51208806, 0.2171649 , 0.19790974, 0.07283729],\n",
      "       [0.49417484, 0.0530624 , 0.17600818, 0.2767546 ],\n",
      "       [0.25975078, 0.11972722, 0.22209626, 0.3984258 ],\n",
      "       [0.05215175, 0.8222115 , 0.07323202, 0.05240474],\n",
      "       [0.91224736, 0.00974044, 0.06678426, 0.01122794],\n",
      "       [0.56449217, 0.12772839, 0.21423455, 0.09354493],\n",
      "       [0.15771988, 0.16402103, 0.19932565, 0.47893348],\n",
      "       [0.07457786, 0.7540506 , 0.09870991, 0.07266164],\n",
      "       [0.4024237 , 0.40386826, 0.15433417, 0.03937383],\n",
      "       [0.25728902, 0.23898083, 0.2072349 , 0.2964952 ],\n",
      "       [0.93807375, 0.00465487, 0.04918053, 0.00809077],\n",
      "       [0.04183402, 0.8432126 , 0.06687161, 0.04808175],\n",
      "       [0.17549194, 0.20994031, 0.21033329, 0.4042345 ],\n",
      "       [0.1965938 , 0.28494465, 0.22086123, 0.29760033],\n",
      "       [0.04252256, 0.84881586, 0.06601896, 0.0426426 ],\n",
      "       [0.19249792, 0.42889595, 0.19648264, 0.18212341],\n",
      "       [0.94688976, 0.00335273, 0.04188817, 0.00786935],\n",
      "       [0.977047  , 0.00121129, 0.01970157, 0.00204011],\n",
      "       [0.9676783 , 0.00185347, 0.02750921, 0.00295907],\n",
      "       [0.27091506, 0.46448445, 0.18088451, 0.08371601],\n",
      "       [0.33639503, 0.14260666, 0.25988573, 0.26111263],\n",
      "       [0.88110745, 0.01994448, 0.08073334, 0.01821473]], dtype=float32), array([[0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.3683459 , 0.36158544, 0.15171005, 0.11835865],\n",
      "       [0.3683459 , 0.36158544, 0.15171005, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.2921851 , 0.298664  , 0.21056716, 0.1985838 ],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.3683459 , 0.36158544, 0.15171005, 0.11835865],\n",
      "       [0.3683459 , 0.36158544, 0.15171005, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.3683459 , 0.36158544, 0.15171005, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.36158544, 0.15171003, 0.11835865],\n",
      "       [0.3683459 , 0.36158544, 0.15171005, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.3683459 , 0.36158544, 0.15171005, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.3683459 , 0.36158544, 0.15171005, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615855 , 0.15171005, 0.11835865],\n",
      "       [0.36834583, 0.3615854 , 0.15171003, 0.11835865],\n",
      "       [0.36834583, 0.3615855 , 0.15171003, 0.11835865]], dtype=float32), array([[0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260384, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.32103395, 0.3026599 , 0.19660655, 0.17969966],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260384, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260384, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260384, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260384, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260384, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260384, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260387, 0.16432683, 0.13369043],\n",
      "       [0.36937886, 0.33260384, 0.16432683, 0.13369043]], dtype=float32), array([[nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan],\n",
      "       [nan, nan, nan, nan]], dtype=float32), array([[0.5635423 , 0.122781  , 0.17493595, 0.13874078],\n",
      "       [0.4306396 , 0.00523553, 0.2981876 , 0.26593727],\n",
      "       [0.7802337 , 0.0221136 , 0.03007538, 0.1675774 ],\n",
      "       [0.20005341, 0.01030183, 0.12554832, 0.6640964 ],\n",
      "       [0.5286636 , 0.23648673, 0.16645443, 0.06839525],\n",
      "       [0.5663442 , 0.04819339, 0.16266517, 0.22279724],\n",
      "       [0.5198544 , 0.31542048, 0.1281951 , 0.03653007],\n",
      "       [0.17580517, 0.6197023 , 0.14146045, 0.06303208],\n",
      "       [0.4540773 , 0.06222593, 0.19030574, 0.29339093],\n",
      "       [0.18361671, 0.6989158 , 0.08899922, 0.02846826],\n",
      "       [0.14175665, 0.7197445 , 0.10781239, 0.03068645],\n",
      "       [0.17580517, 0.6197023 , 0.14146045, 0.06303208],\n",
      "       [0.6644538 , 0.03398024, 0.18618579, 0.11538021],\n",
      "       [0.54989016, 0.047873  , 0.19799323, 0.2042436 ],\n",
      "       [0.22432752, 0.6812678 , 0.07545893, 0.01894581],\n",
      "       [0.67847437, 0.06094594, 0.15988606, 0.10069364],\n",
      "       [0.5664219 , 0.15847486, 0.20142488, 0.07367834],\n",
      "       [0.2796914 , 0.6250753 , 0.0692673 , 0.02596604],\n",
      "       [0.14175665, 0.7197445 , 0.10781239, 0.03068645],\n",
      "       [0.7111832 , 0.00379437, 0.03896884, 0.24605359],\n",
      "       [0.14747953, 0.71583235, 0.09568907, 0.04099903],\n",
      "       [0.7167738 , 0.05191968, 0.13550866, 0.09579787],\n",
      "       [0.73771906, 0.06448023, 0.14709891, 0.05070183],\n",
      "       [0.4080255 , 0.06367038, 0.24075611, 0.28754795],\n",
      "       [0.4175151 , 0.41733867, 0.11849862, 0.04664761],\n",
      "       [0.59956145, 0.21005584, 0.13505009, 0.05533262],\n",
      "       [0.39446753, 0.0257255 , 0.07247874, 0.5073282 ],\n",
      "       [0.5162981 , 0.17374244, 0.21706133, 0.09289815],\n",
      "       [0.408712  , 0.40606204, 0.131439  , 0.05378697],\n",
      "       [0.49244896, 0.07129531, 0.20953335, 0.22672245],\n",
      "       [0.15303905, 0.7321988 , 0.0929805 , 0.02178169],\n",
      "       [0.16112259, 0.7204508 , 0.09387273, 0.02455381],\n",
      "       [0.36146635, 0.46028388, 0.13836701, 0.03988276],\n",
      "       [0.8013662 , 0.0022789 , 0.04292497, 0.15342994],\n",
      "       [0.233163  , 0.6604781 , 0.08231573, 0.02404314],\n",
      "       [0.6115548 , 0.20185535, 0.13664554, 0.04994431],\n",
      "       [0.42562032, 0.42919102, 0.10916309, 0.03602557],\n",
      "       [0.40138948, 0.42641023, 0.1410454 , 0.03115496],\n",
      "       [0.5347073 , 0.05906324, 0.15604554, 0.2501839 ],\n",
      "       [0.4799088 , 0.34010413, 0.13626388, 0.0437232 ],\n",
      "       [0.15661244, 0.73244154, 0.09047318, 0.02047286],\n",
      "       [0.14175665, 0.7197445 , 0.10781239, 0.03068645],\n",
      "       [0.5833606 , 0.20515135, 0.1584855 , 0.0530025 ],\n",
      "       [0.8196057 , 0.01845225, 0.08478683, 0.07715518],\n",
      "       [0.4967758 , 0.09350219, 0.19853006, 0.21119192],\n",
      "       [0.6150773 , 0.18422845, 0.15509088, 0.04560333],\n",
      "       [0.39115056, 0.41930512, 0.13752055, 0.05202377],\n",
      "       [0.54557055, 0.11084456, 0.23309152, 0.11049339],\n",
      "       [0.6926732 , 0.061668  , 0.15564047, 0.09001838],\n",
      "       [0.14175665, 0.7197445 , 0.10781239, 0.03068645],\n",
      "       [0.54062057, 0.25366974, 0.14245453, 0.06325518],\n",
      "       [0.59740573, 0.12808676, 0.20709765, 0.06740987],\n",
      "       [0.6918696 , 0.04934108, 0.1574037 , 0.10138566],\n",
      "       [0.3293994 , 0.40705302, 0.19126345, 0.07228413],\n",
      "       [0.17116977, 0.7064781 , 0.09893413, 0.02341806],\n",
      "       [0.6044236 , 0.14597306, 0.17171547, 0.07788789],\n",
      "       [0.597738  , 0.06004114, 0.26053634, 0.08168452],\n",
      "       [0.13491043, 0.7224091 , 0.10849471, 0.0341857 ],\n",
      "       [0.43372986, 0.05520561, 0.16638489, 0.3446796 ],\n",
      "       [0.39390993, 0.07873735, 0.25820836, 0.2691444 ],\n",
      "       [0.14592838, 0.69423   , 0.11727964, 0.04256199],\n",
      "       [0.41492844, 0.10346572, 0.23166363, 0.2499422 ],\n",
      "       [0.64837074, 0.06082854, 0.2120268 , 0.07877396],\n",
      "       [0.4845158 , 0.34585673, 0.13042653, 0.03920091],\n",
      "       [0.4933789 , 0.00565417, 0.10018516, 0.40078178],\n",
      "       [0.50973195, 0.32066762, 0.12775634, 0.04184406],\n",
      "       [0.21838349, 0.00963557, 0.1868346 , 0.5851463 ],\n",
      "       [0.27875197, 0.6271384 , 0.06961258, 0.0244971 ]], dtype=float32), array([[6.02558196e-01, 1.41982153e-01, 1.21271364e-01, 1.34188235e-01],\n",
      "       [6.07253551e-01, 1.90063038e-05, 6.08686218e-03, 3.86640489e-01],\n",
      "       [5.37306786e-01, 9.34730172e-02, 1.55586451e-01, 2.13633761e-01],\n",
      "       [7.40059793e-01, 1.00555196e-02, 5.20994850e-02, 1.97785184e-01],\n",
      "       [4.62929875e-01, 3.03562522e-01, 1.48494944e-01, 8.50125849e-02],\n",
      "       [5.13578296e-01, 2.22179309e-01, 1.60439089e-01, 1.03803322e-01],\n",
      "       [3.80357713e-01, 4.03333992e-01, 1.44343033e-01, 7.19652474e-02],\n",
      "       [2.69164622e-01, 4.60561097e-01, 1.68541968e-01, 1.01732381e-01],\n",
      "       [5.57667017e-01, 1.41981512e-01, 1.77521199e-01, 1.22830294e-01],\n",
      "       [2.90517658e-01, 5.06322682e-01, 1.28961071e-01, 7.41986036e-02],\n",
      "       [2.76327014e-01, 4.73690450e-01, 1.61025152e-01, 8.89573395e-02],\n",
      "       [2.56423354e-01, 4.63686913e-01, 1.74849421e-01, 1.05040252e-01],\n",
      "       [5.20305097e-01, 9.05152857e-02, 2.37228960e-01, 1.51950642e-01],\n",
      "       [4.77396458e-01, 2.12640136e-01, 1.77800938e-01, 1.32162422e-01],\n",
      "       [3.40775430e-01, 4.68520790e-01, 1.17178328e-01, 7.35253617e-02],\n",
      "       [6.29377782e-01, 1.16116874e-01, 1.22792751e-01, 1.31712601e-01],\n",
      "       [3.57953697e-01, 3.60207081e-01, 1.81373626e-01, 1.00465611e-01],\n",
      "       [3.42722833e-01, 4.24210846e-01, 1.48230612e-01, 8.48356783e-02],\n",
      "       [2.56923825e-01, 5.07359147e-01, 1.43437088e-01, 9.22799110e-02],\n",
      "       [1.54571215e-04, 2.26763002e-22, 1.03093978e-09, 9.99845386e-01],\n",
      "       [2.67239928e-01, 4.83335078e-01, 1.50943398e-01, 9.84815806e-02],\n",
      "       [5.91137946e-01, 9.44216102e-02, 1.38335928e-01, 1.76104531e-01],\n",
      "       [4.90101933e-01, 1.62057176e-01, 2.10654750e-01, 1.37186244e-01],\n",
      "       [4.92121637e-01, 2.48773277e-01, 1.40603691e-01, 1.18501365e-01],\n",
      "       [4.24252868e-01, 3.47832382e-01, 1.43787891e-01, 8.41268301e-02],\n",
      "       [5.07172167e-01, 2.99493700e-01, 1.27637550e-01, 6.56966195e-02],\n",
      "       [5.42449057e-01, 7.99714327e-02, 2.26784214e-01, 1.50795341e-01],\n",
      "       [4.53996807e-01, 3.23916197e-01, 1.31380290e-01, 9.07066837e-02],\n",
      "       [4.61084098e-01, 3.53484541e-01, 1.08803883e-01, 7.66275004e-02],\n",
      "       [4.79631215e-01, 1.56872287e-01, 1.95016205e-01, 1.68480262e-01],\n",
      "       [3.23781937e-01, 4.79284078e-01, 1.25339568e-01, 7.15943649e-02],\n",
      "       [2.72102654e-01, 4.87438321e-01, 1.46168202e-01, 9.42908078e-02],\n",
      "       [3.40597332e-01, 4.41561520e-01, 1.31874785e-01, 8.59663934e-02],\n",
      "       [9.47676301e-01, 4.79045093e-06, 8.15789856e-04, 5.15031293e-02],\n",
      "       [3.13464612e-01, 4.81232554e-01, 1.28300712e-01, 7.70021081e-02],\n",
      "       [4.23573196e-01, 3.51342857e-01, 1.40784830e-01, 8.42991471e-02],\n",
      "       [3.56093615e-01, 4.50881928e-01, 1.27065271e-01, 6.59591481e-02],\n",
      "       [3.15777034e-01, 4.44794357e-01, 1.51968747e-01, 8.74599293e-02],\n",
      "       [5.40652514e-01, 1.54780194e-01, 1.84566244e-01, 1.20001078e-01],\n",
      "       [3.70485812e-01, 4.22949135e-01, 1.28944680e-01, 7.76203722e-02],\n",
      "       [2.61992127e-01, 5.09422064e-01, 1.35563388e-01, 9.30223539e-02],\n",
      "       [2.65576810e-01, 4.99883920e-01, 1.39910623e-01, 9.46286991e-02],\n",
      "       [4.31947321e-01, 3.42419177e-01, 1.43365994e-01, 8.22674930e-02],\n",
      "       [5.92635691e-01, 8.45178962e-02, 1.75823435e-01, 1.47022963e-01],\n",
      "       [5.17651677e-01, 5.58619313e-02, 2.11421177e-01, 2.15065226e-01],\n",
      "       [4.22247589e-01, 3.45843226e-01, 1.45525411e-01, 8.63837972e-02],\n",
      "       [4.02632743e-01, 4.02662635e-01, 1.07567012e-01, 8.71376619e-02],\n",
      "       [5.10553777e-01, 2.72632986e-01, 1.38445303e-01, 7.83679262e-02],\n",
      "       [5.53311169e-01, 1.07771657e-01, 1.76972374e-01, 1.61944792e-01],\n",
      "       [2.60493875e-01, 5.15770972e-01, 1.33004546e-01, 9.07305703e-02],\n",
      "       [4.23680335e-01, 3.52861136e-01, 1.37425944e-01, 8.60326141e-02],\n",
      "       [4.64639246e-01, 2.81329483e-01, 1.58157095e-01, 9.58741978e-02],\n",
      "       [5.73389351e-01, 1.78097829e-01, 1.52420282e-01, 9.60924998e-02],\n",
      "       [3.67427200e-01, 4.49826777e-01, 1.20152578e-01, 6.25934824e-02],\n",
      "       [2.85993189e-01, 4.98393774e-01, 1.38373822e-01, 7.72392377e-02],\n",
      "       [4.48562235e-01, 3.27463955e-01, 1.40295371e-01, 8.36784616e-02],\n",
      "       [5.44569969e-01, 1.39286801e-01, 2.02121884e-01, 1.14021368e-01],\n",
      "       [2.74666786e-01, 5.05706251e-01, 1.28061011e-01, 9.15659517e-02],\n",
      "       [5.24830401e-01, 1.88093767e-01, 1.72139406e-01, 1.14936382e-01],\n",
      "       [4.74946290e-01, 2.56717831e-01, 1.69320360e-01, 9.90155190e-02],\n",
      "       [2.71410793e-01, 4.79810536e-01, 1.47124782e-01, 1.01653971e-01],\n",
      "       [4.83903319e-01, 3.26076299e-01, 1.15867935e-01, 7.41524100e-02],\n",
      "       [4.99035090e-01, 1.46808907e-01, 1.79688796e-01, 1.74467236e-01],\n",
      "       [3.96468073e-01, 4.12094265e-01, 1.12137936e-01, 7.92997852e-02],\n",
      "       [7.91720808e-01, 1.39512442e-04, 4.01992444e-03, 2.04119816e-01],\n",
      "       [4.02016044e-01, 3.72893363e-01, 1.42818585e-01, 8.22720006e-02],\n",
      "       [6.44210637e-01, 2.09128410e-02, 1.16921946e-01, 2.17954606e-01],\n",
      "       [3.45606148e-01, 4.84175175e-01, 1.13695376e-01, 5.65232448e-02]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(yhat1)\n",
    "yhats=[yhat1, yhat2, yhat3, yhat4, yhat5, yhat6]\n",
    "print(yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0-gpu",
   "language": "python",
   "name": "tf2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

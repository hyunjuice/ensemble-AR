{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning for activity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.layers import GRU, LSTM, Activation, Bidirectional\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten \n",
    "from keras.layers import Dense, concatenate\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from numpy import dstack\n",
    "from attention import Attention\n",
    "\n",
    "# univariate cnn lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import ConvLSTM2D\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from itertools import product\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X: (340, 724)\n",
      "The shape of Y: (340, 4)\n"
     ]
    }
   ],
   "source": [
    "# load dataset with pandas\n",
    "dataframe1 = pd.read_csv('data/input_v2/Presentation_activity.csv', sep=',', header=None)\n",
    "dataframe2 = pd.read_csv('data/input_v2/Chatting_activity.csv', sep=',', header=None)\n",
    "dataframe3 = pd.read_csv('data/input_v2/Discussion_activity.csv', sep=',', header=None)\n",
    "dataframe4 = pd.read_csv('data/input_v2/GroupStudy_activity.csv', sep=',', header=None)\n",
    "\n",
    "# Show how many episodes \n",
    "#print(len(dataframe1))\n",
    "#print(len(dataframe2))\n",
    "#print(len(dataframe3))\n",
    "#print(len(dataframe4))\n",
    "\n",
    "#Combine all dataframe\n",
    "dataframe = dataframe1.append(dataframe2,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe3,ignore_index=True)\n",
    "dataframe = dataframe.append(dataframe4,ignore_index=True)\n",
    "\n",
    "#Change NaN data to 0\n",
    "dataframe = dataframe.fillna(0)\n",
    "dataset = dataframe.values\n",
    "\n",
    "#288, 6\n",
    "#Seperate input and do one-hot encoding\n",
    "#X=to_categorical(dataset)\n",
    "X=dataset\n",
    "print(\"The shape of X:\", X.shape)\n",
    "\n",
    "# Construct Y label\n",
    "arr = [] #  empty regular list\n",
    "for i in range(len(dataframe1)):\n",
    "    arr.append(0*np.ones((1)))\n",
    "for i in range(len(dataframe2)):\n",
    "    arr.append(1*np.ones((1)))\n",
    "for i in range(len(dataframe3)):\n",
    "    arr.append(2*np.ones((1)))\n",
    "for i in range(len(dataframe4)):\n",
    "    arr.append(3*np.ones((1)))\n",
    "np_array = np.array(arr)  # transformed to a numpy array\n",
    "y_df = pd.DataFrame({'': np_array[:, 0]})\n",
    "\n",
    "Y=to_categorical(y_df)\n",
    "#Y=y_df\n",
    "print(\"The shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train DeepLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "max_len=724\n",
    "batch_size = 64\n",
    "length=724\n",
    "n_members=7\n",
    "n_features = 1\n",
    "n_seq = 4\n",
    "n_steps = 181\n",
    "def model1_made():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Embedding(8, 128, input_length=max_len, mask_zero = True))\n",
    "    model1.add(SpatialDropout1D(0.2))\n",
    "    model1.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model1.add(Dense(4, activation='softmax'))\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model1\n",
    "def model2_made():\n",
    "    model2 = Sequential()\n",
    "    model2.add(Embedding(8, 128, input_length=max_len, mask_zero = True))\n",
    "    model2.add(SpatialDropout1D(0.2))  \n",
    "    model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model2.add(Dense(4, activation='softmax'))\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model2\n",
    "def model3_made():\n",
    "    model3 = Sequential()\n",
    "    model3.add(Embedding(8, 128, input_length=max_len, mask_zero = True))\n",
    "    model3.add(SpatialDropout1D(0.2))  \n",
    "    model3.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model3.add(Dense(4, activation='softmax'))\n",
    "    model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model3\n",
    "def model4_made():\n",
    "    model4 = Sequential()\n",
    "    model4.add(Embedding(8, 128, input_length=max_len, mask_zero = True))\n",
    "    model4.add(SpatialDropout1D(0.2))  \n",
    "    model4.add(Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model4.add(Dense(4, activation='softmax'))\n",
    "    model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model4\n",
    "def model5_made():\n",
    "    # define model\n",
    "    model5 = Sequential()\n",
    "    model5.add(TimeDistributed(Conv1D(filters=4, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "    model5.add(TimeDistributed(MaxPooling1D(pool_size=4)))\n",
    "    model5.add(TimeDistributed(Flatten()))\n",
    "    model5.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model5.add(Dense(4, activation='softmax'))\n",
    "    model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model5\n",
    "def model6_made():\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=4, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=4)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model7_made():\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=4, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def model8_made():\n",
    "    max_len=724\n",
    "    sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences = Embedding(8, 128, input_length=max_len, mask_zero = True)(sequence_input)\n",
    "\n",
    "    lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences = True))(embedded_sequences)\n",
    "    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional \\\n",
    "      (LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)\n",
    "\n",
    "    print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n",
    "\n",
    "    state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
    "    state_c = Concatenate()([forward_c, backward_c]) # 셀 상태\n",
    "\n",
    "    attention = BahdanauAttention(64) # 가중치 크기 정의\n",
    "    context_vector, attention_weights = attention(lstm, state_h)\n",
    "\n",
    "    dense1 = Dense(20, activation=\"relu\")(context_vector)\n",
    "    dropout = Dropout(0.2)(dense1)\n",
    "    output = Dense(4, activation=\"softmax\")(dropout)\n",
    "    model = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "def model_evaluate1(X_train,Y_train,model,X_test, Y_test):\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # evaluate model\n",
    "    accr = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    #plt.title('Loss')\n",
    "    #plt.plot(history.history['loss'], label='train')\n",
    "    #plt.plot(history.history['val_loss'], label='test')\n",
    "    #plt.legend()\n",
    "    #plt.show();\n",
    "\n",
    "    #plt.title('Accuracy')\n",
    "    #plt.plot(history.history['accuracy'], label='train')\n",
    "    #plt.plot(history.history['val_accuracy'], label='test')\n",
    "    #plt.legend()\n",
    "    #plt.show();\n",
    "\n",
    "    yhat=model.predict(X_test,verbose=0)\n",
    "    #argmax(yhat1, axis=1)\n",
    "    return [accr, yhat]\n",
    "def model_evaluate2(X_train,Y_train,model,X_test, Y_test):\n",
    "    # fit model\n",
    "    X_train1 = X_train.reshape((X_train.shape[0], n_seq, n_steps, n_features))\n",
    "    model.fit(X_train1, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # demonstrate prediction    \n",
    "    X_test1 = X_test.reshape((X_test.shape[0], n_seq, n_steps, n_features))\n",
    "    accr = model.evaluate(X_test1, Y_test, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    yhat = model.predict(X_test1, verbose=0)\n",
    "    return [accr, yhat]\n",
    "def model_evaluate3(X_train,Y_train,model,X_test, Y_test):    \n",
    "    # fit model\n",
    "    X_train2 = X_train.reshape((X_train.shape[0], n_seq, 1, n_steps, n_features))\n",
    "    Y_train2 = Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))\n",
    "    model.fit(X_train2, Y_train2, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # demonstrate prediction    \n",
    "    X_test2 = X_test.reshape((X_test.shape[0], n_seq, 1, n_steps, n_features))\n",
    "    Y_test2 = Y_test.reshape((Y_test.shape[0], 1, Y_test.shape[1]))\n",
    "    accr = model.evaluate(X_test2, Y_test2, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "        \n",
    "    yhat = model.predict(X_test2, verbose=0)\n",
    "    yhat = np.reshape(yhat,(len(yhat),4))\n",
    "    return [accr, yhat]\n",
    "def model_evaluate4(X_train,Y_train,model,X_test, Y_test):    \n",
    "    # fit model\n",
    "    Y_train = argmax(Y_train, axis=1)\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    # demonstrate prediction    \n",
    "    Y_test = argmax(Y_test, axis=1)\n",
    "    accr = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "        \n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    return [accr, yhat]\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "\t# calculate l1 vector norm\n",
    "\tresult = norm(weights, 1)\n",
    "\t# check for a vector of all zeros\n",
    "\tif result == 0.0:\n",
    "\t\treturn weights\n",
    "\t# return normalized vector (unit norm)\n",
    "\treturn weights / result\n",
    "\n",
    "def top2ensemble(accr,yhats,Y_test):\n",
    "    largest,second_largest=0,0\n",
    "    for i in range(len(accr)):\n",
    "        if accr[i] > accr[largest]:\n",
    "            largest = i\n",
    "        elif accr[largest] > accr[i] > accr[second_largest]:\n",
    "                second_largest = i\n",
    "\n",
    "    pre_rs = [(yhats[largest][i] + yhats[second_largest][i]) for i in range(len(yhats[0]))]     \n",
    "    pre_rs = argmax(pre_rs, axis=1)    \n",
    "    true_rs=argmax(Y_test, axis=1)\n",
    "    count=0\n",
    "    for j in range(len(true_rs)):\n",
    "        if pre_rs[j]==true_rs[j] :\n",
    "            count+=1       \n",
    "    print('Top2Ensemble accuracy : ', count/len(true_rs))    \n",
    "\n",
    "def ensempb(accr,yhats,Y_test):\n",
    "    pre_rs = [(yhats[0][i] + yhats[1][i] +yhats[2][i] +yhats[3][i]+yhats[4][i]+yhats[5][i]+yhats[6][i]) for i in range(len(yhats[0]))] \n",
    "    pre_rs=argmax(pre_rs, axis=1)\n",
    "    true_rs=argmax(Y_test, axis=1)    \n",
    "    count=0\n",
    "    for j in range(len(true_rs)):\n",
    "        if pre_rs[j]==true_rs[j] :\n",
    "            count+=1\n",
    "    print('Probability ensemble accuracy : ', count/len(true_rs))\n",
    "def ensemvote(yhats,Y_test):\n",
    "    pre_rs = np.full((len(Y_test),4),0)\n",
    "    rs = np.zeros((n_members, len(Y_test)))\n",
    "    for i in range(n_members):\n",
    "        rs[i]= argmax(yhats[i], axis=1)\n",
    "\n",
    "    for i in range(n_members):\n",
    "        for j in range(len(Y_test)):\n",
    "            pre_rs[j][int(rs[i][j])]+=1\n",
    "    pre_rs = argmax(pre_rs, axis=1)\n",
    "    true_rs = argmax(Y_test, axis=1)\n",
    "\n",
    "    count=0\n",
    "    for j in range(len(true_rs)):\n",
    "        if pre_rs[j]==true_rs[j] :\n",
    "            count+=1\n",
    "    print('Voting ensemble accuracy : ', count/len(true_rs))   \n",
    "    \n",
    "def ensemweighted(accr,yhats,Y_test):\n",
    "    r2=argmax(Y_test, axis=1)\n",
    "    # define weights to consider\n",
    "    w = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    best_score, best_weights = 0.0, None\n",
    "    for weights in product(w, repeat=len(members)):\n",
    "        # skip if all weights are equal\n",
    "        if len(set(weights)) == 1:\n",
    "            continue\n",
    "    # hack, normalize weight vector\n",
    "        #weights = normalize(weights)\n",
    "    # evaluate weights\n",
    "        yhats =  array(yhats)\n",
    "        summed =  tensordot(yhats, weights, axes=((0),(0)))\n",
    "        result = argmax(summed, axis=1)\n",
    "        score =  accuracy_score(r2, result)\n",
    "        if score > best_score:\n",
    "            best_score, best_weights = score, weights\n",
    "    \n",
    "    print('>%s %.3f' % (best_weights, best_score))\n",
    "    \n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = Dense(units)\n",
    "    self.W2 = Dense(units)\n",
    "    self.V = Dense(1)\n",
    "\n",
    "  def call(self, values, query): # 단, key와 value는 같음\n",
    "    # query shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_8658 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 17s - loss: 1.3802 - accuracy: 0.3177Executing op __inference_keras_scratch_graph_9130 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 94s 384ms/step - loss: 1.3782 - accuracy: 0.3197 - val_loss: 1.3492 - val_accuracy: 0.3929\n",
      "Test set\n",
      "  Loss: 1.347\n",
      "  Accuracy: 0.515\n",
      "Executing op __inference_keras_scratch_graph_9590 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_11360 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 9s - loss: 1.3798 - accuracy: 0.3125 Executing op __inference_keras_scratch_graph_11641 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 49s 200ms/step - loss: 1.3750 - accuracy: 0.3320 - val_loss: 1.3499 - val_accuracy: 0.3571\n",
      "Test set\n",
      "  Loss: 1.343\n",
      "  Accuracy: 0.500\n",
      "Executing op __inference_keras_scratch_graph_11910 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_13733 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 9s - loss: 1.3847 - accuracy: 0.2708 Executing op __inference_keras_scratch_graph_14033 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 49s 202ms/step - loss: 1.3783 - accuracy: 0.3074 - val_loss: 1.3530 - val_accuracy: 0.3571\n",
      "Test set\n",
      "  Loss: 1.347\n",
      "  Accuracy: 0.412\n",
      "Executing op __inference_keras_scratch_graph_14321 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_17406 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 18s - loss: 1.3825 - accuracy: 0.2344Executing op __inference_keras_scratch_graph_17916 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 95s 391ms/step - loss: 1.3766 - accuracy: 0.2664 - val_loss: 1.3379 - val_accuracy: 0.3571\n",
      "Test set\n",
      "  Loss: 1.338\n",
      "  Accuracy: 0.529\n",
      "Executing op __inference_keras_scratch_graph_18414 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_21002 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/244 [======================>.......] - ETA: 12s - loss: 1.3724 - accuracy: 0.3021 Executing op __inference_keras_scratch_graph_21379 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "244/244 [==============================] - 64s 262ms/step - loss: 1.3500 - accuracy: 0.3361 - val_loss: 1.2935 - val_accuracy: 0.3571\n",
      "Test set\n",
      "  Loss: 1.293\n",
      "  Accuracy: 0.412\n",
      "Executing op __inference_keras_scratch_graph_21744 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Train on 244 samples, validate on 28 samples\n",
      "Epoch 1/1\n",
      "Executing op __inference_keras_scratch_graph_23367 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      " 64/244 [======>.......................] - ETA: 1:08 - loss: 1.3992 - accuracy: 0.2188Executing op __inference_keras_scratch_graph_23622 in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "for k in range(1):\n",
    "    accr=[]    \n",
    "    # Divide train and test data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    members=[model1_made(),model2_made(),model3_made(),model4_made(),model5_made(),model6_made(),model7_made()]\n",
    "    result =[model_evaluate1(X_train,Y_train,members[0],X_test, Y_test),\n",
    "           model_evaluate1(X_train,Y_train,members[1],X_test, Y_test),\n",
    "           model_evaluate1(X_train,Y_train,members[2],X_test, Y_test),\n",
    "           model_evaluate1(X_train,Y_train,members[3],X_test, Y_test),\n",
    "           model_evaluate2(X_train,Y_train,members[4],X_test, Y_test),\n",
    "           model_evaluate2(X_train,Y_train,members[5],X_test, Y_test),\n",
    "           model_evaluate3(X_train,Y_train,members[6],X_test, Y_test)]\n",
    "    \n",
    "    yhats= np.zeros((len(result), len(Y_test), 4))    \n",
    "    for i in range(len(result)):\n",
    "        yhats[i]=result[i][1]\n",
    "        accr.append(result[i][0][1])\n",
    "    print('Base Model Accuracy : ',accr)\n",
    "\n",
    "    top2ensemble(accr,yhats,Y_test)\n",
    "    ensempb(accr,yhats,Y_test)\n",
    "    ensemvote(yhats,Y_test)\n",
    "    #ensemweighted(accr,yhats,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest,second_largest=0,0\n",
    "for i in range(len(accr)):\n",
    "    if accr[i] > accr[largest]:\n",
    "        largest = i\n",
    "    elif accr[largest] > accr[i] > accr[second_largest]:\n",
    "            second_largest = i\n",
    "\n",
    "pre_rs = [(yhats[largest][i] + yhats[second_largest][i]) for i in range(len(yhats[0]))]     \n",
    "pre_rs = argmax(pre_rs, axis=1)    \n",
    "print(pre_rs)\n",
    "    \n",
    "true_rs=argmax(Y_test, axis=1)\n",
    "print(true_rs)\n",
    "    \n",
    "count=0\n",
    "for j in range(len(true_rs)):\n",
    "    if pre_rs[j]==true_rs[j] :\n",
    "        count+=1       \n",
    "\n",
    "print(count/len(true_rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_rs = [(yhats[0][i] + yhats[1][i] +yhats[2][i] +yhats[3][i]+yhats[4][i]+yhats[5][i]) for i in range(len(yhats[0]))] \n",
    "\n",
    "pre_rs=argmax(pre_rs, axis=1)\n",
    "print(pre_rs)\n",
    "\n",
    "true_rs=argmax(Y_test, axis=1)\n",
    "print(true_rs)\n",
    "\n",
    "count=0\n",
    "for j in range(len(true_rs)):\n",
    "    if pre_rs[j]==true_rs[j] :\n",
    "        count+=1\n",
    "print(count/len(true_rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_rs = np.full((len(Y_test),4),0)\n",
    "rs = np.zeros((n_members, len(Y_test)))\n",
    "for i in range(n_members):\n",
    "    rs[i]= argmax(yhats[i], axis=1)\n",
    "        \n",
    "for i in range(n_members):\n",
    "    for j in range(len(Y_test)):\n",
    "        pre_rs[j][int(rs[i][j])]+=1\n",
    "pre_rs = argmax(pre_rs, axis=1)\n",
    "true_rs = argmax(Y_test, axis=1)\n",
    "\n",
    "count=0\n",
    "for j in range(len(true_rs)):\n",
    "    if pre_rs[j]==true_rs[j] :\n",
    "        count+=1\n",
    "print('Voting ensemble accuracy : ', count/len(true_rs))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "\t# calculate l1 vector norm\n",
    "\tresult = norm(weights, 1)\n",
    "\t# check for a vector of all zeros\n",
    "\tif result == 0.0:\n",
    "\t\treturn weights\n",
    "\t# return normalized vector (unit norm)\n",
    "\treturn weights / result\n",
    "r2=argmax(Y_test, axis=1)\n",
    "\n",
    "# define weights to consider\n",
    "w = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "best_score, best_weights = 0.0, None\n",
    "for weights in product(w, repeat=len(members)):\n",
    "\t# skip if all weights are equal\n",
    "\tif len(set(weights)) == 1:\n",
    "\t\tcontinue\n",
    "# hack, normalize weight vector\n",
    "\tweights = normalize(weights)\n",
    "# evaluate weights\n",
    "\tyhats =  array(yhats)\n",
    "\tsummed =  tensordot(yhats, weights, axes=((0),(0)))\n",
    "\tresult = argmax(summed, axis=1)\n",
    "\tscore =  accuracy_score(r2, result)\n",
    "\tif score > best_score:\n",
    "\t\tbest_score, best_weights = score, weights\n",
    "\t\tprint('>%s %.3f' % (best_weights, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhat1)\n",
    "yhats=[yhat1, yhat2, yhat3, yhat4, yhat5, yhat6]\n",
    "print(yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0-gpu",
   "language": "python",
   "name": "tf2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
